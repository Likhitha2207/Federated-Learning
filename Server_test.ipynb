{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5386de6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48f7878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eb888db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_input.txt', encoding='utf-8') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01241f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 28, 29, 1, 30, 31, 32, 33, 2, 34, 35, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup=BeautifulSoup(data,'html.parser')#remove html tag using beautifulsoup\n",
    "non_html_text=soup.get_text()\n",
    "    \n",
    "#remove unwanted charectors and symbols\n",
    "text=re.sub('[^a-zA-Z0-9\\s]',' ',non_html_text)\n",
    "text = text.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "\n",
    "#remove extra spaces\n",
    "z = []\n",
    "for i in text.split():\n",
    "    if i not in z:\n",
    "        z.append(i)  \n",
    "text = ' '.join(z)\n",
    "\n",
    "#tokenize text\n",
    "    \n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "sequence=tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "sequence[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecca4060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "695"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip_dim=len(tokenizer.word_index)+1\n",
    "ip_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e090df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3, 10)             6950      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 3, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 695)               695695    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13751645 (52.46 MB)\n",
      "Trainable params: 13751645 (52.46 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Embedding\n",
    "model=Sequential()\n",
    "model.add(Embedding(ip_dim,10,input_length=3))\n",
    "model.add(LSTM(1000,return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(ip_dim,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20ea3ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f1f452e-8cc7-40fc-8c81-fbd742e2390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('server.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8937fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef federated_averaging(global_weights, client_weights_list):\\n    num_clients = len(client_weights_list)\\n    averaged_weights = global_weights.copy()\\n    \\n    for i in range(len(averaged_weights)):\\n        # Initialize with the global weights\\n        averaged_weights[i] = global_weights[i]\\n        \\n        # Aggregate model updates from all clients\\n        for client_weights in client_weights_list:\\n            averaged_weights[i] += client_weights[i] / num_clients\\n    \\n    return averaged_weights\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def federated_averaging(global_weights, client_weights_list):\n",
    "    num_clients = len(client_weights_list)\n",
    "    averaged_weights = global_weights.copy()\n",
    "    \n",
    "    for i in range(len(averaged_weights)):\n",
    "        # Initialize with the global weights\n",
    "        averaged_weights[i] = global_weights[i]\n",
    "        \n",
    "        # Aggregate model updates from all clients\n",
    "        for client_weights in client_weights_list:\n",
    "            averaged_weights[i] += client_weights[i] / num_clients\n",
    "    \n",
    "    return averaged_weights\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d9ca022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define the server\\'s IP address and port\\nserver_ip = \\'192.168.94.137\\'  # Replace with your server\\'s local IP address\\nserver_port = 12347\\n\\n# Create a server socket\\nserver_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n\\n# Bind the server socket to the IP address and port\\nserver_socket.bind((server_ip, server_port))\\n\\n# Listen for incoming connections\\nserver_socket.listen(3)\\n\\nprint(f\"Server is listening for incoming connections on {server_ip}:{server_port}...\")\\n\\n\\n# Accept connections from multiple clients\\n# Simulate federated learning rounds\\nnum_rounds = 6\\nnum_clients= 3\\n    \\n# Receive model weights from each client\\nclient_weights_list = []\\n\\nfor _ in range(num_clients):\\n    client_socket, client_address = server_socket.accept()\\n    print(f\"Accepted connection from {client_address}\")\\n    \\n    client_weights_bytes = b\\'\\'\\n    for round_num in range(num_rounds):\\n        while True:\\n            data = client_socket.recv(13246848)  # Adjust buffer size as needed\\n            if not data:\\n                break\\n            client_weights_bytes += data\\n            \\n\\n        if not client_weights_bytes:\\n            print(f\"No data received from a client{_ + 1}, in round {round_num}.\")\\n            continue\\n\\n        try:\\n            client_model_weights = pickle.loads(client_weights_bytes)\\n            client_weights_list.append(client_model_weights)\\n            \\n        except pickle.UnpicklingError as e:\\n            print(f\"Error while unpickling data from a client: {e}\")\\n            \\n    client_socket.close()\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Define the server's IP address and port\n",
    "server_ip = '192.168.94.137'  # Replace with your server's local IP address\n",
    "server_port = 12347\n",
    "\n",
    "# Create a server socket\n",
    "server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# Bind the server socket to the IP address and port\n",
    "server_socket.bind((server_ip, server_port))\n",
    "\n",
    "# Listen for incoming connections\n",
    "server_socket.listen(3)\n",
    "\n",
    "print(f\"Server is listening for incoming connections on {server_ip}:{server_port}...\")\n",
    "\n",
    "\n",
    "# Accept connections from multiple clients\n",
    "# Simulate federated learning rounds\n",
    "num_rounds = 6\n",
    "num_clients= 3\n",
    "    \n",
    "# Receive model weights from each client\n",
    "client_weights_list = []\n",
    "\n",
    "for _ in range(num_clients):\n",
    "    client_socket, client_address = server_socket.accept()\n",
    "    print(f\"Accepted connection from {client_address}\")\n",
    "    \n",
    "    client_weights_bytes = b''\n",
    "    for round_num in range(num_rounds):\n",
    "        while True:\n",
    "            data = client_socket.recv(13246848)  # Adjust buffer size as needed\n",
    "            if not data:\n",
    "                break\n",
    "            client_weights_bytes += data\n",
    "            \n",
    "\n",
    "        if not client_weights_bytes:\n",
    "            print(f\"No data received from a client{_ + 1}, in round {round_num}.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            client_model_weights = pickle.loads(client_weights_bytes)\n",
    "            client_weights_list.append(client_model_weights)\n",
    "            \n",
    "        except pickle.UnpicklingError as e:\n",
    "            print(f\"Error while unpickling data from a client: {e}\")\n",
    "            \n",
    "    client_socket.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97d2ad92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Aggregate model updates using federated averaging\\nif client_weights_list:\\n    global_weights = federated_averaging(model.get_weights(), client_weights_list)\\n    model.set_weights(global_weights)\\n    print(f\"Updated global model with data from {num_clients} clients in round {len(client_weights_list)}.\")\\n\\n        \\n# The global model now contains the federated learning result\\nmodel.save_weights(\\'model_weights.h5\\')\\n\\n# Close the server socket\\nserver_socket.close()\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Aggregate model updates using federated averaging\n",
    "if client_weights_list:\n",
    "    global_weights = federated_averaging(model.get_weights(), client_weights_list)\n",
    "    model.set_weights(global_weights)\n",
    "    print(f\"Updated global model with data from {num_clients} clients in round {len(client_weights_list)}.\")\n",
    "\n",
    "        \n",
    "# The global model now contains the federated learning result\n",
    "model.save_weights('model_weights.h5')\n",
    "\n",
    "# Close the server socket\n",
    "server_socket.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d79c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# For preprocessing input text for test\\ndef preprocess(text):\\n    soup1=BeautifulSoup(text,'html.parser')#remove html tag using beautifulsoup\\n    non_html_txt=soup.get_text()\\n    \\n    #remove unwanted charectors and symbols\\n    text=re.sub('[^a-zA-Z0-9\\\\s]',' ',non_html_txt)\\n    text = text.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\\n\\n    #remove extra spaces\\n    z = []\\n    for i in text.split():\\n        if i not in z:\\n            z.append(i)  \\n    text = ' '.join(z)\\n    return text\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# For preprocessing input text for test\n",
    "def preprocess(text):\n",
    "    soup1=BeautifulSoup(text,'html.parser')#remove html tag using beautifulsoup\n",
    "    non_html_txt=soup.get_text()\n",
    "    \n",
    "    #remove unwanted charectors and symbols\n",
    "    text=re.sub('[^a-zA-Z0-9\\s]',' ',non_html_txt)\n",
    "    text = text.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "\n",
    "    #remove extra spaces\n",
    "    z = []\n",
    "    for i in text.split():\n",
    "        if i not in z:\n",
    "            z.append(i)  \n",
    "    text = ' '.join(z)\n",
    "    return text\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0432c515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# It gives single predicted words\\ndef predict_nxt_word(txt):\\n\\n    tok_text = tokenizer.texts_to_sequences([ txt])\\n    #print(tok_text)\\n    preds = model.predict(np.array(tok_text), verbose=0)[0]\\n    next_word = tokenizer.sequences_to_texts([[np.argmax(preds)]])[0]\\n    return next_word\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# It gives single predicted words\n",
    "def predict_nxt_word(txt):\n",
    "\n",
    "    tok_text = tokenizer.texts_to_sequences([ txt])\n",
    "    #print(tok_text)\n",
    "    preds = model.predict(np.array(tok_text), verbose=0)[0]\n",
    "    next_word = tokenizer.sequences_to_texts([[np.argmax(preds)]])[0]\n",
    "    return next_word\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a45fe9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntxt='Can I'\\ntxt=preprocess(txt)\\ntxt=txt.split(' ')\\ntxt=txt[-3:]\\n# print(txt)\\nnxt=predict_nxt_word(txt)\\nprint(nxt)\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "txt='Can I'\n",
    "txt=preprocess(txt)\n",
    "txt=txt.split(' ')\n",
    "txt=txt[-3:]\n",
    "# print(txt)\n",
    "nxt=predict_nxt_word(txt)\n",
    "print(nxt)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee2aa013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#formula caluclation\\ndata_size = 79481088  # Bytes\\nnum_iterations = 6\\n\\nbuffer_size = data_size // num_iterations\\n\\nprint(buffer_size)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#formula caluclation\n",
    "data_size = 79481088  # Bytes\n",
    "num_iterations = 6\n",
    "\n",
    "buffer_size = data_size // num_iterations\n",
    "\n",
    "print(buffer_size)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eda761c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
